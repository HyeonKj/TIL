=================
# 2022-03-23
=================

## 거리와 유사도 - 여러가지 거리  

* 유클리드 거리 = 문서의 길이에 따라 유사도를 구분하기 때문에 오류를 범할 수 있다. 
* 맨하탄 거리 
* 레벤슈타인 거리 = 두 문자열을 같게 만들기 위해 필요한 수정의 수 (오타를 수정하기 위해) 
* 해밍거리 = 길이가 같은 문자열에서 같은 위치에 다른 글자가 있는 경우  

=================
## 자카드 유사도 jaccard similarity
#### 사람 얼굴인식할 때 얼마나 유사한지 평가할 때 쓰이는 평가

============
## 코사인 유사도 = 두 문서 간의 유사도를 계산할 때 흔히 사용, 

============
# 단어 임베딩 = 컴퓨터가 자연어를 이해할 수 있도록 변환시키는 과정. 숫자로. 

============
## 사용되는 용어 
### * 희소 표현 / 원핫벡터
### * 밀집 표현 / Dense /  
### * 워드임베딩 / 
### 단어 -> 랜덤값(밀집벡터) 변환 
##### Embedding() 

================
### word2vec 특징 =  아웃풋이 빨리 나오도록 학습하는 것에 초점. 
### 단어에 대한 임베딩을 빠르게 하기 위해서 주변값을 통해 가운뎃값을 추정한다는지 등의 방법을 쓴다. 
### skip-gram(전->중<- or 전<-중->, 앞뒤 맥락으로 예측하는 것)이 cbow보다 성능이 높음. 
### cbow가 더 쉽습니다만 임베딩 성능은 skip-gram이 더 좋다.

## CBOW(continous Bag of words)
## Skip-gram 차이 
================
## 2022 - 03 - 24 
===============
# WORD2Vec / 2013년에 만들어졌다 
워드 투 벡은 단어 임베딩의 한 종류이다. 
단어 임베딩을 효율적으로 계산할 수 있도록 신경망 언어 모형을 수정 
밀집 벡터 (원핫인코딩은 저장 공간이 너무 크다보니까 밀집시켜서 표현하자.)

================
- Fast Text
### Word2Vec을 개선 
- 글자 단위 n-gram을 이용한 준단어 토큰화를 이용
    - n=3일 경우 where는 <wh, whe, her, ere, re>로 분해함
- 단어 임베딩 = 준단어 토큰의 임베딩의 합 

================
