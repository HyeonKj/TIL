==============
# 04-05 식목일
==============
# 언어 모형 Language models

- 언어 모형이란 문장의 확률을 계산하기 위한 모형이다. 

p(x1, x2, ... xn)
ex. 밥을 먹었다. vs 밥을 마셨다.

=====================
# 확률의 연쇄규칙

- 결합확률 : x1과 x2가 동시에 발생할 확률 (교집합)

- 조건부확률 :x1이 있는 상태에서 x2가 나타날 확률 

- 연쇄규칙 : P(X1,X2) = P(X2|X1)P(X1) 조건부확률을 연쇄규칙으로 나타낼 수 있다. 

===============
# 인과적 언어 모형 (causal language models)
- 결합 확률 대신 조건부 확률 형태의 언어 모형.

- '인과적 언어모형보다 단순히 ' 언어모형 ' 이라고 하는 경우가 많다.

- 인공신경망 등의 모형으로 구현하기 쉬움/

- 단어를 순서대로 생성할 수 있음. 

===============
# n-gram 언어모형

- 텍스트에서 최대 n개의 단어 조합의 빈도를 세서 언어 모형을 간단히 구현할 수 있음.

- n 단어 이상의 맥락은 고려할 수 없음

- storage problem : n이 커질수록 조합이 폭발적으로 증가하여 많은 저장 공간이 필요함.
- sparsity problem : 텍스트가 충분히 많지 않으면 대부분의 조합은 빈도가 0 

==============
# 신경망 언어 모형 (Neural Network Language Model)

- n 개의 단어를 입력으로 받음 x1, x2, x3 ... xn

- 각 단어 xi를 원핫 인코딩
- 단어를 임베딩 
- 앞먹임 신경망 ( 출력층 - 은닉층 - 임베딩 - 원핫 인코딩)
- 다음에 나올 xn번째 단어를 예측 


===============
## 단어 임베딩 word embedding
- 단어를 m 차원의 조밀한 벡터로 표현 (m << n)
- 의미적, 문법적으로 유사한 단어는 비슷한 값을 가지도록 함
- 신경망에서는 원핫인코딩된 입력 x에 가중치 행렬 W를 곱하는 레이어로 구현 

=================
# 신경망 언어모형의 장점과 한계 : n - gram vs. NNLM
- storage problem : 
    - n-gram은 어휘의 종류가 w개 이면 w의 n승개의 조합을 저장.
    - NNLM은 임베딩의 크기가 e, 은닉층의  +크기가 h일 경우 we + eh + hw의 파라미터만 필요

- sparsity problem:(희소성)
    - NNLM에서는 비슷한 단어는 비슷한 임베딩을 갖게 됨
    - 말뭉치에 '떡을 먹다'라는 문장이 없어도 '밥을 먹다', '빵을 먹다' 등의 사례를 통해 '떡을 먹다;에 높은 확률을 주도록 학습할 수 있음. 

================
## NNLM의 한계

- n-gram과 마찬가지로 n개의 단어까지만 반영됨.
- 단어의 위치에 따라 가중치가 달라짐
- n을 키우면 모형이 커짐

============
# 순환신경망과 주의 매커니즘 : 사라지는 경사 또는 폭발하는 경사

- 신경망은 경사하강법을 통해 학습
- 동일한 가중치 W가 여러 번 곱해지므로 
- 점점 작아져서 사라지거나 -- > 학습이 안됨
- 점점 커져서 폭발하는 문자 -- > 학습이 불안정 

![순환신경망](https://t2.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/17Xk/image/CaG8aFbUMK9sF6ce-JQewa125sE.png)

===========================================
## 순환신경망과 주의 매커니즘 : LSTM의 한계

- 순차적으로 계산되기 때문에 병렬처리가 어려움
- 문장이 길러질수록 네트워크도 길어져서 학습시키기 어려움
