{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing_TPOT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN2hIYTe+WrH4SxKkbeNOyZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonKj/TIL/blob/master/Testing_TPOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPOT test"
      ],
      "metadata": {
        "id": "7-oQqzEf6E9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1mu3WaJ6Cmr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #드라이브에서 파일 가지고 오기 \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#불러올 파일의 경로를 filename 변수에 저장\n",
        "filename = '/content/drive/MyDrive/전복나이예측대회/dataset/train.csv'\n",
        "\n",
        "#pandas read_csv로 불러오기\n",
        "inputData = pd.read_csv(filename)\n",
        "display(inputData.sample(10))"
      ],
      "metadata": {
        "id": "-TBifogrAdpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 추후 원핫인코딩으로 변경 \n",
        "\n",
        "sexEncoded = []\n",
        "for i in range(len(inputData[\"Gender\"].values)):\n",
        "    if inputData[\"Gender\"].values[i] == \"M\":\n",
        "        value=1\n",
        "    elif inputData[\"Gender\"].values[i] == \"F\":\n",
        "        value=0\n",
        "    else:\n",
        "        value=-1\n",
        "    sexEncoded.append(value)\n",
        "inputData[\"Gender\"] = sexEncoded\n",
        "\n",
        "display(inputData.describe())"
      ],
      "metadata": {
        "id": "XUrlp_8TAdvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputData.shape"
      ],
      "metadata": {
        "id": "20RXouE5Ady6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputData.dtypes"
      ],
      "metadata": {
        "id": "ZUFayNjNBMdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputData.info())"
      ],
      "metadata": {
        "id": "YGqceT9uBpL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test dataset\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/전복나이예측대회/dataset/test.csv')\n",
        "test_df.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "# transform the gender feature as a onehot encoded value\n",
        "test_df = pd.get_dummies(test_df)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "CKYmWuZdM3Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [데이콘 공유된 코드 참고](https://dacon.io/competitions/official/235877/codeshare/4698?page=1&dtype=recent)\n"
      ],
      "metadata": {
        "id": "6Rtvh-05VATi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = inputData['Target']\n",
        "# convert to numpy array\n",
        "X_train = inputData.values\n",
        "y_train = y_train.values\n",
        "X_test = test_df.values\n",
        "print(X_train.shape, y_train.shape, X_test.shape)\n",
        "\n",
        "# (1253, 10) (1253,) (2924, 10)"
      ],
      "metadata": {
        "id": "JohHbtLxNERX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scale features - standard scaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# scale target - minmax + median normalization\n",
        "# -> to be zero centered\n",
        "y_train_scaled = (y_train - y_train.min()) * 2 / (y_train.max() - y_train.min())\n",
        "median = np.median(y_train_scaled)\n",
        "y_train_scaled = y_train_scaled - median"
      ],
      "metadata": {
        "id": "0TJw2LXmNz-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram of scaled target\n",
        "plt.hist(y_train_scaled, bins=20)\n",
        "plt.title('Histogram of scaled target')\n",
        "plt.text(1, 70, f'Mean: {y_train_scaled.mean():.2f}')\n",
        "plt.text(1, 50, f'Std: {y_train_scaled.std():.2f}')\n",
        "plt.text(1, 30, f'Median: {np.median(y_train_scaled):.2f}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LMpUEUk8N-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "weight_decay = 1e-5\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# build a simple FNN using keras\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='tanh', input_shape=(X_train.shape[1],), kernel_regularizer=keras.regularizers.l2(weight_decay)),\n",
        "    # keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(64, activation='tanh', kernel_regularizer=keras.regularizers.l2(weight_decay)),\n",
        "    # keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='tanh' ,kernel_regularizer=keras.regularizers.l2(weight_decay)),\n",
        "    # keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate) , loss='mae', metrics=['mae'])\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "metadata": {
        "id": "CbTcTZ1iN-rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled, y_train_scaled, validation_split=0.1,  epochs=1000, batch_size=128, callbacks=[early_stopping, lr_scheduler], verbose=0)\n",
        "plt.plot(history.history['loss'][3:], label='train')\n",
        "plt.plot(history.history['val_loss'][3:], label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(history.history['val_loss'][-30])"
      ],
      "metadata": {
        "id": "wi_T21X7N-tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict test data\n",
        "y_hat = model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "rDehlG8QOUN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse scaling\n",
        "y_hat_unscaled = (y_hat + median) / 2 * (y_train.max() - y_train.min()) + y_train.min()\n",
        "y_hat_unscaled = y_hat_unscaled.astype(int)\n",
        "y_hat_unscaled = np.clip(y_hat_unscaled, 1, 100)"
      ],
      "metadata": {
        "id": "DFCytSR1OUQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram of scaled target\n",
        "plt.hist(y_hat_unscaled, bins=10, label='predicted', alpha=0.5)\n",
        "plt.hist(y_train, bins=10, label='original', alpha=0.5)\n",
        "plt.title('Histogram of predicted and original target')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "33bVYeOYOlz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/전복나이예측대회/dataset/sample_submission.csv')\n",
        "submission['Target'] = y_hat_unscaled\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "#The End"
      ],
      "metadata": {
        "id": "3gPeTjDWOl2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape"
      ],
      "metadata": {
        "id": "h4sSRt2zOp3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.info()"
      ],
      "metadata": {
        "id": "NgdL6mLvOp6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.describe()"
      ],
      "metadata": {
        "id": "Ay7-4kPVQYWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_unscaled.values"
      ],
      "metadata": {
        "id": "FqE-JPJGQYcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1Ne30zXsOUTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = inputData[\"Target\"]\n",
        "X = inputData.copy(deep=True)\n",
        "X.drop([\"Target\"], axis=1, inplace=True)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "X = pd.DataFrame(X)"
      ],
      "metadata": {
        "id": "WS6uXdGzAd1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **코드**  [TPOT 사용](https://www.simonwenkel.com/2019/06/08/revisitingML-abalone.html)"
      ],
      "metadata": {
        "id": "pq4Ue5-fO22C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tpot"
      ],
      "metadata": {
        "id": "DpiCZCJ2Ji27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_log_error, mean_squared_error, mean_absolute_error\n",
        "import sklearn.metrics\n",
        "from tpot import TPOTRegressor\n"
      ],
      "metadata": {
        "id": "DyYSV4whAd4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_squared_log_error, median_absolute_error\n",
        "import sklearn.metrics\n",
        "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# from skgarden.mondrian import MondrianForestRegressor\n",
        "import xgboost\n"
      ],
      "metadata": {
        "id": "H7ZQA1gnzvHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputData = pd.read_csv('/content/drive/MyDrive/전복나이예측대회/dataset/train.csv')\n",
        "sexEncoded = []\n",
        "for i in range(len(inputData[\"Gender\"].values)):\n",
        "    if inputData[\"Gender\"].values[i] == \"M\":\n",
        "        value=1\n",
        "    elif inputData[\"Gender\"].values[i] == \"F\":\n",
        "        value=0\n",
        "    else:\n",
        "        value=-1\n",
        "    sexEncoded.append(value)\n",
        "inputData[\"Gender\"] = sexEncoded\n",
        "\n",
        "y = inputData[\"Target\"]\n",
        "X = inputData.copy(deep=True)\n",
        "X.drop([\"Target\"], axis=1, inplace=True)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "X = pd.DataFrame(X)\n",
        "\n",
        "X_train = inputData.values\n",
        "X_test = test_df.values\n",
        "y_train = inputData['Target']\n",
        "y_test = y_hat_unscaled\n",
        "\n",
        "tpot = TPOTRegressor(max_time_mins=180,\n",
        "                     verbosity=1,\n",
        "                     n_jobs=-1)\n",
        "tpot.fit(X_train,y_train)\n",
        "tpot.export('abalone.py')\n",
        "\n",
        "y_predictions = tpot.predict(X_test)\n",
        "r2 = sklearn.metrics.r2_score(y_test, y_predictions)\n",
        "mae = sklearn.metrics.mean_absolute_error(y_test, y_predictions)\n",
        "mse = sklearn.metrics.mean_squared_error(y_test, y_predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"R2 score:\", r2)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "YAy5Mgb2SMBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}