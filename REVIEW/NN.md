===================
# neural net
===================
케라스에서 만들 모델을 학습할 때는 사이킷런처럼 fit( ) 함수를 이용합니다.
이 때, 케라스에서는 사이킷런과는 달리 batch_size와 epochs 라는 파라미터가 존재합니다.

배치사이즈는 몇 개의 관측치에 대한 예측을 하고, 레이블 값과 비교를 하는지를 설정하는 파라미터입니다. 한번에 넘겨주는 데이터 수 .

에포크는 하나의 데이터셋을 몇 번 반복 학습할지 정하는 파라미터입니다. 
같은 데이터셋이라 할지라도 가중치가 계속해서 업데이트되기 때문에 모델이 추가적으로 학습이 가능합니다. 
반복학습을 통해 모델의 성능을 향상시킬 수 있습니다. 
하지만, 너무 많이 반복학습을 하면 학습셋에 대해 성능은 올라가지만 관측되지 못한 테스트셋에 대한 성능이 떨어지는 오버피팅(overfitting)이 발생하게 됩니다.
 때문에, 오버피팅이 일어날 것 같으면 학습을 종료합니다.(early stopping)

 이때 learning rate를 늘리거나 줄여주는 방법으로 빠져나오는 효과를 기대할 수 있습니다.
Keras에는 콜백함수로 제공하고 있으며, ReduceLROnPlateau이 바로 그 역할을 합니다.

# 콜백 정의 학습률을 개선하기 위해서 학습률을 동적으로 조정하는 콜백함수입니다. 

reduceLR = ReduceLROnPlateau(
    monitor='val_loss',  # 검증 손실을 기준으로 callback이 호출됩니다
    factor=0.5,          # callback 호출시 학습률을 1/2로 줄입니다
    patience=10,         # epoch 10 동안 개선되지 않으면 callback이 호출됩니다

EarlyStopping 콜백을 활용하면, model의 성능 지표가 설정한 epoch동안 개선되지 않을 때 조기 종료할 수 있습니다. EarlyStopping과 이전에 언급한 ModelCheckpoint 콜백의 조합을 통하여, 개선되지 않는 학습에 대한 조기 종료를 실행하고, ModelCheckpoint로 부터 가장 best model을 다시 로드하여 학습을 재게할 수 있습니다.

모니터는 기준 설정, patience는 몇회동안 개선되지 않는다면 종료 min 가장 낮은 best를 찾는 것. 


현재 가장 많이 사용하는 활성화 함수를 뽑자면 ReLU함수를 뽑을 수 있을 것 같다. 

이 함수는 f(x) = max(0, x) 라는 매우 간단한 수식으로 0이하는 0으로 고정하고 0 값을 초과할 경우 해당 값을 그대로 출력하는 매우 심플한 구조이다.

ReLU은 값이 0이하일 땐 모두 0으로 변경을 하기 때문에 Dying ReLU(뉴런이 0을 출력하여 더이상 학습이 안되는 문제)와 같은 문제가 발생할 수 있다. 이를 막고자 나온 개념 중 하나가 ELU function이다.

- relu의 문제점을 해결하기 위해서 생겨난 게 elu함수다. 
출처: https://needjarvis.tistory.com/564 [자비스가 필요해]

