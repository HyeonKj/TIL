===================
# 2022-0329
===================

# 순환 신경망(Recurrent Neural Network) RNN

- 이전 단계의 처리결과가 다음 단계에 입력이 되는 형태의 신경망이다. 

- 동일 처리를 반복하는 형태이므로 길이가 긴 문장을 처리하더라도 파라미터의 수가 늘어나지 않음 
=============
# Unrolling
- 순환신경망을 [출력 - 은닉층 - 입력] 펼쳐놓은 것 
==============
# 바닐라 RNN

==============
# RNN의 장점
- 긴 맥락을 하나의 벡터로 표현
- 맥락을 한 단계로 업데이트 할 수 있음. 


==============
# RNN은 여러 단계에 걸쳐 동일한 처리를 반복하는 형태.
### RNN은 사라지는 경사(Vanishing Gradient) 문제가 있다. 경사가 사라진다는 것은...

==============
# LSTM과 GRU

- RNN의 사라지는 경사 문제를 완화하기 위해 내부 구조를 수정한 모형
- LSTM은 출력 ht와 함께 활성화 함수를 거치지 않는 별도의 신호 Ct-를 순환.
- GRU는 활성화 함수를 거친 신호가 이전의 출력 ht-1과 더해짐
- 순환되는 신호가 활성화 함수를 거치지 않는 것이 공통점

==============
# 순환신경망의 방향
- 정방향(순방향) - 앞에서 뒤로 처리 
- 역방향 - 뒤에서 앞으로 처리 
- 양방향 - 정방향과 역방향으로 각각 처리한 결과를 합+침.

==============
# 순환신경망의 문제점
- 데이터를 앞에서부터 뒤로 순차적으로 계산 
- 병렬 계산이 불가능하므로 계산 속도가 느림 
- LSTM 등을 사용하더라도 긴 문맥의 학습은 잘 되지 않음. 

==============

