# 언어 모형과 n-gram 
===================
## : 자연어 문장의 확률을 계산하는 모형 
### 예) a. 나는 밥을 먹었다. 
###     b. 나는 밥을 마셨다. 
##     a)가 b)보다 확률이 높음. 

# 언어 모형은 다양한 자연어 처리에 활용됨.

## 사전학습(pre-training) : 언어 모형은 텍스트만으로 학습이 가능 
##          - 사전 학습된 언어 모형으로 학습 효율을 높임


## 자연어 생성 :
##              자연어 문장을 생성할 때 확률이 높은 표현을 고를 수 있음. 

=======================
## n-gram =  n개의 토큰을 묶어서 세는 것 

## gram = 글이나 그림(그리스어)instagram, grammer

ex. " i have a dream"
1-gram : i, have, a , dream
2-gram : i have, have a, a dream
3-gram : i have a , have a dream
=================================

### n-gram 언어 모형
- 문장을 이루는 토큰의 확률을 n-gram으로 추정 
- 문장의 확률 = 토큰의 확률의 곱

* n=1
* 모든 토큰이 독립
* 직전에 무슨 토큰이 나오든지 다음 토큰에 영향 X 

